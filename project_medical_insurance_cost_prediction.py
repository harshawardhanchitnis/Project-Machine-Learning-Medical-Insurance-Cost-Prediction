# -*- coding: utf-8 -*-
"""PROJECT - MEDICAL INSURANCE COST PREDICTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K0hzn02e9jUVDnaiGs3PkO-vZzyMNKIp

# **PROJECT - MEDICAL INSURANCE COST PREDICTION**

## Project Description

In this project, we aim to predict **Medical Insurance Costs** using machine learning algorithms. Accurate prediction of insurance costs can help insurance companies provide better pricing models for customers based on specific features.

### Dataset
We will use the [Medical Insurance Cost Dataset](https://github.com/harshawardhanchitnis/Project-Machine-Learning-Medical-Insurance-Cost-Prediction/blob/main/insurance.csv). This dataset includes the following features:

- **Age**: Age of the primary beneficiary.  
- **Sex**: Gender of the beneficiary (male/female).  
- **BMI**: Body mass index, an indicator of whether an individual is underweight, normal weight, overweight, or obese.  
- **Children**: Number of dependents covered by insurance.  
- **Smoker**: Smoking habit of the individual (yes/no).  
- **Region**: Residential area of the individual (northwest, southeast, etc.).  
- **Charges**: The target variable representing individual medical insurance costs.

### Algorithms
We will use the following machine learning algorithms for prediction:

1. **Linear Regression**: A simple algorithm that models the relationship between independent variables and the target variable by fitting a straight line.
2. **Random Forest Regression**: A robust and flexible ensemble learning method that creates multiple decision trees and combines their results for more accurate predictions.

## Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np  # For numerical computations
import pandas as pd  # For data handling
import matplotlib.pyplot as plt  # For plotting
import seaborn as sns  # For advanced visualizations
from sklearn.model_selection import train_test_split  # For splitting the dataset
from sklearn.linear_model import LinearRegression  # Linear Regression Model
from sklearn.ensemble import RandomForestRegressor  # Random Forest Regressor
from sklearn import metrics  # For evaluating model performance
# %matplotlib inline

"""## Reading Dataset"""

url = "https://raw.githubusercontent.com/harshawardhanchitnis/Project-Machine-Learning-Medical-Insurance-Cost-Prediction/main/insurance.csv"
dataset = pd.read_csv(url)

"""## Printing first 5 rows from the dataset"""

dataset.head()

"""## Display dataset information"""

dataset.info()

"""## Display the shape of the dataset - Rows and Columns"""

dataset.shape

"""## Checking for missing values in the dataset

"""

dataset.isnull().sum()

"""## Displaying the columns of the dataset"""

dataset.columns

"""## Visualizing Distribution of Dataset Columns

"""

# Set the figure size
plt.figure(figsize=(12, 10))

# Plotting the distribution for each column in the dataset
for i, column in enumerate(dataset.columns):
    plt.subplot(3, 3, i+1)  # Create subplots in a 3x3 grid
    sns.histplot(dataset[column], kde=True, bins=20)  # Histogram with Kernel Density Estimate (KDE)
    plt.title(f'Distribution of {column}')  # Title for each plot
    plt.tight_layout()

# Show the plots
plt.show()

"""## Correlation Heatmap of Dataset Features"""

# Selecting only the numeric columns from the dataset
numeric_columns = dataset.select_dtypes(include=['float64', 'int64'])

# Calculate the correlation matrix for numeric columns
correlation_matrix = numeric_columns.corr()

# Plotting the heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap of Dataset Features')
plt.show()

"""## Scatterplot: Age vs Charges (Smoker vs Non-Smoker)"""

# Scatterplot Visualization: Age vs Charges (with smoker as hue)
sns.scatterplot(x='age', y='charges', hue='smoker', data=dataset)
plt.title('Age vs Charges (Smoker vs Non-Smoker)')
plt.show()

"""## Scatterplot: BMI vs Charges (Smoker vs Non-Smoker)"""

# Scatterplot Visualization: BMI vs Charges (with smoker as hue)
sns.scatterplot(x='bmi', y='charges', hue='smoker', data=dataset)
plt.title('BMI vs Charges (Smoker vs Non-Smoker)')
plt.show()

"""## Splitting Features and Target Variable"""

# Splitting the dataset into features (X) and target variable (y)
X = dataset.drop(columns="charges", axis=1)
Y = dataset["charges"]

# Displaying the shapes of X and y
print('Shape of X = ', X.shape)
print('Shape of Y = ', y.shape)

"""## Label Encoding for Categorical Variable"""

dataset.replace({'sex': {'male': 1, 'female': 0}}, inplace=True)
dataset.replace({'smoker': {'yes': 1, 'no': 0}}, inplace=True)
dataset.replace({'region': {'southwest': 1, 'southeast': 0, 'northeast': 2, 'northwest': 3}}, inplace=True)

"""## Again Splitting Features and Target Variable"""

#splitting features and target
X= dataset.drop(columns = "charges" , axis = 1)
y = dataset["charges"]

"""## Printing the features and target variable"""

print("Features (X):")
print(X)

print("\nTarget Variable (Y):")
print(y)

"""## Splitting Data into Training and Testing Sets"""

# Import train_test_split from sklearn
from sklearn.model_selection import train_test_split

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

# Print the shapes of the training and testing data
print('Shape of X_train = ', X_train.shape)
print('Shape of y_train = ', y_train.shape)
print('Shape of X_test = ', X_test.shape)
print('Shape of y_test = ', y_test.shape)

"""# **LINEAR REGRESSION MODEL**

## Training and Evaluating Linear Regression Model
"""

# Import Linear Regression from sklearn
# Import necessary libraries
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Create a Linear Regression model
linear_model = LinearRegression()

# Train the model on the training data
linear_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred_linear = linear_model.predict(X_test)

# Calculate R² Score
r2_linear = r2_score(y_test, y_pred_linear)

# Calculate Mean Absolute Error (MAE)
mae_linear = mean_absolute_error(y_test, y_pred_linear)

# Calculate Mean Squared Error (MSE)
mse_linear = mean_squared_error(y_test, y_pred_linear)

# Calculate Root Mean Squared Error (RMSE)
rmse_linear = np.sqrt(mse_linear)

# Print the evaluation metrics
print(f"Linear Regression - R² Score: {r2_linear:.4f}")
print(f"Linear Regression - Mean Absolute Error (MAE): {mae_linear:.2f}")
print(f"Linear Regression - Mean Squared Error (MSE): {mse_linear:.2f}")
print(f"Linear Regression - Root Mean Squared Error (RMSE): {rmse_linear:.2f}")

"""## Building a Prediction System using Multiple Linear Regression (MLR)"""

# Making Predictions using Linear Regression Model
def predict_insurance_cost(age, sex, bmi, children, smoker, region):
    # Create a DataFrame with the input values
    input_data = pd.DataFrame([[age, sex, bmi, children, smoker, region]],
                              columns=['age', 'sex', 'bmi', 'children', 'smoker', 'region'])

    # Predict the charges (insurance cost) using the trained model
    predicted_charge = linear_model.predict(input_data)

    return predicted_charge[0]

# Example of using the prediction system
predicted_cost = predict_insurance_cost(30, 1, 29.0, 2, 1, 1)  # Example input values
print(f"Predicted Insurance Cost: ${predicted_cost:.2f}")

"""### Justification of Insurance Cost Prediction Output
The model predicts an insurance cost of $29,861.34 for the given input values (Age=30, Sex=Male, BMI=29, Children=2, Smoker=Yes, Region=Southwest).

This prediction is based on the relationships the Linear Regression model has learned from the training data.

Age: Older individuals tend to have higher medical costs, which increases the predicted charge.

Sex (Male): Gender can influence medical costs, with males in this dataset associated with specific charge patterns.

BMI: Higher BMI is linked to greater health risks, leading to higher charges.

Children: The number of children has a moderate effect on the cost prediction.

Smoker (Yes): Smokers are generally predicted to have higher medical charges due to associated health risks.

Region (Southwest): The region also impacts charges due to geographical healthcare cost variations.

***This predicted amount is a reflection of the combined effects of these factors, as learned by the model during training.***

## Plotting for Linear Regression
"""

# Make predictions on the test data using Linear Regression
y_pred_linear = linear_model.predict(X_test)

# Plotting actual vs predicted values for 'age' feature in Linear Regression
plt.scatter(X_test['age'].values, y_test, color='blue', label='Actual Values')
plt.scatter(X_test['age'].values, y_pred_linear, color='red', label='Predicted Values')
plt.title('Age vs Charges (Linear Regression)')
plt.xlabel('Age')
plt.ylabel('Charges')
plt.legend()
plt.show()

# Plotting actual vs predicted values for 'bmi' feature in Linear Regression
plt.scatter(X_test['bmi'].values, y_test, color='blue', label='Actual Values')
plt.scatter(X_test['bmi'].values, y_pred_linear, color='red', label='Predicted Values')
plt.title('BMI vs Charges (Linear Regression)')
plt.xlabel('BMI')
plt.ylabel('Charges')
plt.legend()
plt.show()

"""## Residuals Plot for Linear Regression"""

# Residuals plot for Linear Regression
residuals_linear = y_test - y_pred_linear
plt.scatter(y_pred_linear, residuals_linear, color='blue', label='Linear Regression')
plt.axhline(y=0, color='black', linestyle='--')
plt.title('Residuals Plot (Linear Regression)')
plt.xlabel('Predicted Charges')
plt.ylabel('Residuals')
plt.legend()
plt.show()

"""# **RANDOM FOREST REGRESSION MODEL :**

## Training and Evaluating Random Forest Regression Model
"""

# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Create a Random Forest Regressor model
rf_model = RandomForestRegressor(n_estimators=100, random_state=2)

# Train the model on the training data
rf_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred_rf = rf_model.predict(X_test)

# Calculate R² Score
r2_rf = r2_score(y_test, y_pred_rf)

# Calculate Mean Absolute Error (MAE)
mae_rf = mean_absolute_error(y_test, y_pred_rf)

# Calculate Mean Squared Error (MSE)
mse_rf = mean_squared_error(y_test, y_pred_rf)

# Calculate Root Mean Squared Error (RMSE)
rmse_rf = np.sqrt(mse_rf)

# Print the evaluation metrics
print(f"Random Forest Regression - R² Score: {r2_rf:.4f}")
print(f"Random Forest Regression - Mean Absolute Error (MAE): {mae_rf:.2f}")
print(f"Random Forest Regression - Mean Squared Error (MSE): {mse_rf:.2f}")
print(f"Random Forest Regression - Root Mean Squared Error (RMSE): {rmse_rf:.2f}")

"""## Prediction System Using Random Forest Regression"""

# Making Predictions using Random Forest Regression Model
def predict_insurance_cost_rfr(age, sex, bmi, children, smoker, region):
    # Create a DataFrame with the input values
    input_data = pd.DataFrame([[age, sex, bmi, children, smoker, region]],
                              columns=['age', 'sex', 'bmi', 'children', 'smoker', 'region'])

    # Predict the charges (insurance cost) using the trained Random Forest model
    predicted_charge = rf_model.predict(input_data)

    return predicted_charge[0]

# Example of using the prediction system
predicted_cost_rfr = predict_insurance_cost_rfr(30, 1, 29.0, 2, 1, 1)  # Example input values
print(f"Predicted Insurance Cost (RFR): ${predicted_cost_rfr:.2f}")

"""### Justification for Predicted Insurance Cost (RFR)
The predicted insurance cost of $19,235.44 using the Random Forest Regression (RFR) model for the input values (age=30, sex=1, bmi=29.0, children=2, smoker=1, region=1) reflects the model's learned patterns from the training data.

Key Factors:

Smoker (1, indicating a smoker) significantly increases the cost due to health risks.

BMI of 29.0, which is above the healthy range, also contributes to higher costs.

Children (2) adds a moderate increase due to family coverage.

Age, sex, and region provide additional minor adjustments based on trends learned by the model.

***This prediction is the result of the Random Forest model averaging the predictions of many decision trees, offering a robust and reliable estimate.***

## Feature Importance Plot (Random Forest)
"""

# Feature Importance Plot for Random Forest
importances = rf_model.feature_importances_
features = ['age', 'sex', 'bmi', 'children', 'smoker', 'region']  # Update with the correct feature names
indices = np.argsort(importances)

plt.title('Feature Importance (Random Forest Regression)')
plt.barh(range(len(importances)), importances[indices], align='center')
plt.yticks(range(len(importances)), np.array(features)[indices])
plt.xlabel('Relative Importance')
plt.show()

"""## Plotting for Random Forest Regression"""

# Make predictions on the test data using Random Forest Regression
y_pred_rf = rf_model.predict(X_test)

# Plotting actual vs predicted values for 'age' feature in Random Forest Regression
plt.scatter(X_test['age'].values, y_test, color='blue', label='Actual Values')
plt.scatter(X_test['age'].values, y_pred_rf, color='red', label='Predicted Values')
plt.title('Age vs Charges (Random Forest Regression)')
plt.xlabel('Age')
plt.ylabel('Charges')
plt.legend()
plt.show()

# Plotting actual vs predicted values for 'bmi' feature in Random Forest Regression
plt.scatter(X_test['bmi'].values, y_test, color='blue', label='Actual Values')
plt.scatter(X_test['bmi'].values, y_pred_rf, color='red', label='Predicted Values')
plt.title('BMI vs Charges (Random Forest Regression)')
plt.xlabel('BMI')
plt.ylabel('Charges')
plt.legend()
plt.show()

"""## Residuals Plot for Random Forest Regression"""

# Residuals plot for Random Forest Regression
residuals_rf = y_test - y_pred_rf
plt.scatter(y_pred_rf, residuals_rf, color='red', label='Random Forest Regression')
plt.axhline(y=0, color='black', linestyle='--')
plt.title('Residuals Plot (Random Forest Regression)')
plt.xlabel('Predicted Charges')
plt.ylabel('Residuals')
plt.legend()
plt.show()

"""# **Model Comparison: Actual vs Predicted for Both Models**"""

# Compare predicted vs actual values for both models
plt.scatter(y_test, y_pred_linear, color='blue', label='Linear Regression')
plt.scatter(y_test, y_pred_rf, color='red', label='Random Forest Regression')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='black', linestyle='--', label='Perfect Prediction')
plt.title('Actual vs Predicted (Comparison of Models)')
plt.xlabel('Actual Charges')
plt.ylabel('Predicted Charges')
plt.legend()
plt.show()

"""# **Conclusion and Summary of Findings**

## **In this project, we used two different regression models—Linear Regression (MLR) and Random Forest Regression (RFR)—to predict insurance charges based on various features. Through careful evaluation using R² scores, RMSE, and residual plots, we observed that Random Forest Regression performed better in terms of prediction accuracy, as seen from its higher R² score and lower RMSE. We also visualized the models' predictions and compared them to the actual values, which further confirmed the superior performance of the Random Forest model.**

# **CLOSING REMARKS**

## **This project allowed me to apply regression techniques to a real-world dataset and evaluate model performance using various metrics and visualizations. By using both Linear Regression and Random Forest Regression, I was able to compare the strengths and weaknesses of each model in predicting insurance costs. The project was an excellent opportunity to practice data preprocessing, model training, evaluation, and visualization skills, which are crucial for machine learning and data science.**
"""